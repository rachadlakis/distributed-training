{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "65e6ae75",
      "metadata": {},
      "source": [
        "- Resise the  wallpaper images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "0da8c998",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed: 01e52a92d0e3cb9fb6a657c55c7460bc.jpg to (360, 640)\n",
            "Processed: 06eeac84800ac84990d278356e039c02.jpg to (236, 177)\n",
            "Processed: 1-tribal-by-famous-photographer-jimmy-nelson.jpg to (640, 504)\n",
            "Processed: 1059226577.jpg to (300, 168)\n",
            "Processed: 11739387264gsq17q3lhgadmuwb2b5oey7um71mltpkkryblikq7rpkl0iedcb3qcvd16dwau2op5pl1bvh0bxbp5geixrdgsoinvnmf6ktrx3s.jpg to (358, 640)\n",
            "Processed: 1482585-4k-wallpaper-nature-3840x2160-for-tablet.jpg to (640, 360)\n",
            "Processed: 161866.jpg to (560, 315)\n",
            "Processed: 1689186895552.png to (640, 358)\n",
            "Processed: 168dd17138de7314a8d1ebd32c024523.jpg to (359, 640)\n",
            "Processed: 178777-amoled-atmosphere-ecoregion-nature-afterglow-7680x4320.png to (640, 360)\n",
            "Processed: 180107bcunflags-editorial.jpg to (594, 334)\n",
            "Processed: 1dc2c8e1b09ba7227f9ef026c0812718.jpg to (640, 400)\n",
            "Processed: 1e700e552694c01e854b82c2c9.jpg to (640, 487)\n",
            "Processed: 1_dXbQCIzoU84Yt_077fEKCg.jpg to (640, 274)\n",
            "Processed: 1_pu-k-GYEhttQ3J_9jZB8-Q.jpg to (640, 274)\n",
            "Processed: 230224144031-07-best-beaches-world-tripadvisor-2023-radhanagar-beach.jpg to (640, 360)\n",
            "Processed: 2383088-Judith-Orloff-Quote-Surrender-to-life-today-Don-t-fight-anything.jpg to (640, 360)\n",
            "Processed: 2734c4ba307dfa08df46c0b8095a56a9.jpg to (441, 640)\n",
            "Processed: 274b0146d9364205b9d8335515adf911.jpg to (359, 640)\n",
            "Processed: 276558-wallpaper-720-1440.jpg to (300, 600)\n",
            "Processed: 29415981_150844952414506_7170336798457987072_n-830x553.jpg to (640, 426)\n",
            "Processed: 3277642f444d174e24d94fb15cce6589.jpg to (360, 640)\n",
            "Processed: 360_F_1217415186_DFJ6ca0FPyRHOvOmknx1EtvbFJnoKFVu.jpg to (640, 358)\n",
            "Processed: 360_F_360332122_koNY6CqUBdVT4Y6a1j3m3BwYh5rsq1zR.jpg to (640, 360)\n",
            "Processed: 360_F_439890152_sYbPxa1ANTSKcZuUsKzRAf9O7bJ1Tx5B.jpg to (640, 360)\n",
            "Processed: 360_F_560351415_erJATcR4vNGlAGKtXtlnObnCzVBvMm2M.jpg to (540, 360)\n",
            "Processed: 360_F_713646395_Of7w7Xs9TVq3xY0j6mblxRBJz6tb7iDX.jpg to (540, 360)\n",
            "Processed: 3946211aed83b4036a817a7e0fb851e8.jpg to (640, 360)\n",
            "Processed: 39ee7f4ec3fa453c361b10f28107d799.jpg to (360, 640)\n",
            "Processed: 4107.jpg to (640, 360)\n",
            "Processed: 41f5569046ebf64160d43d2092dff979.jpg to (360, 640)\n",
            "Processed: 43-437846_best-wallpapers-and-backgrounds.jpg to (640, 400)\n",
            "Processed: 4311dca3e20ec5ad8c111a15bc7e20cb.jpg to (640, 360)\n",
            "Processed: 49059-sunset-nature-dark-hd-4k.jpg to (640, 426)\n",
            "Processed: 4k-tech-ulcajgzzc25jlrgi.jpg to (640, 360)\n",
            "Processed: 515d9d7b2d03241915e6ffb5a94b5252.jpg to (360, 640)\n",
            "Processed: 532216-wallpaper-forest-mountains-river-waterfall-canada-albert-alberta-canada-jasper-national-park-sunwapta-falls-sunwapta-river-jasper-image-for-desktop-section-piejzazi.jpg to (640, 408)\n",
            "Processed: 56dc95d48ee1ad729890755050f91c7e.jpg to (640, 360)\n",
            "Processed: 5af155146aa956cd237cc7608dbcb079.jpg to (360, 640)\n",
            "Processed: 659277.jpg to (640, 360)\n",
            "Processed: 674d9f64c8a3c0110654ebdd1e037503.jpeg to (426, 640)\n",
            "Processed: 6c2d4457e7b227254fbc0f51b8.jpg to (640, 426)\n",
            "Processed: 6ea0d1d24ce58444a1b65ea92a42c49e.jpg to (640, 360)\n",
            "Processed: 71ODr8OGjNL.png to (384, 640)\n",
            "Processed: 72415f8c-3e06-40fc-b5a6-32fd76c7b567-shutterstock-1270572721.jpg to (640, 336)\n",
            "Processed: 73dd3bcbffbf8080296d7a289d251620.jpg to (626, 626)\n",
            "Processed: 756e7177772be07c38a25a1e32655433 (1).jpg to (360, 640)\n",
            "Processed: 756e7177772be07c38a25a1e32655433.jpg to (360, 640)\n",
            "Processed: 783258-Kung-Fu-Panda-3-Best-Animation-Movies-of-2015-cartoon.jpg to (640, 360)\n",
            "Processed: 7b6edcf9e93cb91586c2af01a3ddbcf1.jpg to (360, 640)\n",
            "Processed: 7cc23184bd30f71574a68df82e9a61d2.jpg to (325, 640)\n",
            "Processed: 7f71e43fbf013f75c6d153ccbb78e615.jpg to (640, 360)\n",
            "Processed: 7_chakra_meaning_colours_1200x.png to (640, 360)\n",
            "Processed: 8df2b9ee3edf89ae1928c4e8e98eb5fd.jpg to (360, 640)\n",
            "Processed: 9Foto020.jpg to (452, 299)\n",
            "Processed: A-futuristic-cyberpunk-city-skyline-perfect-for-8K-PC-desktop-backgrounds.png to (640, 360)\n",
            "Processed: a1170cc85b8c7f099d6c4e242c2c5bda.jpg to (236, 419)\n",
            "Processed: a4343a6d95ec5fefbdd41f2a7e98be3a.jpg to (640, 425)\n",
            "Processed: ac15bf5577c0111953c4095035e4bdfa.jpg to (359, 640)\n",
            "Processed: aebcf6ac5e33f1a9.png to (597, 435)\n",
            "Processed: Apple-Logo-Window-Dark-Broken-iphone.jpg to (360, 640)\n",
            "Processed: b2af927ee1b856f0cdf05bbd386bbf58.jpg to (303, 640)\n",
            "Processed: b33dfc5d47461c40511bb1e9343971ba.jpg to (360, 640)\n",
            "Processed: baby-groot-4k-hd-superheroes-wallpaper-preview.jpg to (640, 360)\n",
            "Processed: beautiful-domestic-cat-laying-fence.jpg to (640, 427)\n",
            "Processed: beautiful-shot-grassy-hills-covered-trees-near-mountains-dolomites-italy.jpg to (640, 426)\n",
            "Processed: best-hd-wallpapers-for-pc-free-download.jpg to (640, 373)\n",
            "Processed: best-photos-2021-hiroki-nose-autumn-snake.jpeg to (512, 640)\n",
            "Processed: bfdaee6919733ae01b9036f888165cc7.jpg to (640, 408)\n",
            "Processed: black-pictures_044739662.jpg to (640, 360)\n",
            "Processed: blue-flower-desktop-29z7zs36vc5xi10c.jpg to (640, 352)\n",
            "Processed: boy-meditating-life-triangle-4k-vw.jpg to (640, 360)\n",
            "Processed: Buddha.jpeg to (287, 640)\n",
            "Processed: c3a6d286a01f8bb83d1376be902ca45a.jpg to (480, 360)\n",
            "Processed: caaeb4e75111db782709a5e40dab2a55.jpg to (236, 373)\n",
            "Processed: calm-water-nature-dark-wallpaper-preview.jpg to (640, 360)\n",
            "Processed: cfd1b6bd308c0a969f3f4ea06ad91ce2.jpg to (360, 640)\n",
            "Processed: Child.jpeg to (330, 640)\n",
            "Processed: Chinatown-New-York-United-States-iphone.jpg to (360, 640)\n",
            "Processed: city 2.jpeg to (482, 640)\n",
            "Processed: city.jpeg to (288, 640)\n",
            "Processed: cityscape-building-city-sunrise-wallpaper-preview.jpg to (512, 640)\n",
            "Processed: clouds green.jpeg to (288, 640)\n",
            "Processed: clouds.jpeg to (295, 640)\n",
            "Processed: Colorful-Black-Wallpapers.jpg to (640, 400)\n",
            "Processed: colors 2.jpg to (630, 360)\n",
            "Processed: colors.jpg to (275, 183)\n",
            "Processed: cool-background-wallpaper-hd-1080p.jpg to (640, 360)\n",
            "Processed: dark-minimal-scenery-4k-xj.jpg to (640, 349)\n",
            "Processed: dark-nature-rj2zws3ww99n2jzb.jpg to (640, 360)\n",
            "Processed: dark-nature-s3in5w7pgukadq51.jpg to (640, 360)\n",
            "Processed: dark-shiva-silhouette-and-stars-77u1tt4e7sgzteui.jpg to (320, 640)\n",
            "Processed: dark-souls-knight-720x1280-18742.jpeg to (360, 640)\n",
            "Processed: depositphotos_380509576-stock-photo-abstract-meditation-man-chakras-golden.jpg to (600, 600)\n",
            "Processed: desktop-wallpaper-amazing-lion-scary-lions-thumbnail.jpg to (350, 622)\n",
            "Processed: desktop-wallpaper-black-and-red-iphone-black-red-phone-thumbnail.jpg to (350, 622)\n",
            "Processed: desktop-wallpaper-dark-spiral-bokeh-boquet-light-bulb-electricity.jpg to (640, 392)\n",
            "Processed: desktop-wallpaper-earth-orbit-earth-ultra.jpg to (640, 400)\n",
            "Processed: desktop-wallpaper-iphone-7-full-1080-1920-awesome-hi-quality-within-iphone-7-thumbnail.jpg to (350, 622)\n",
            "Processed: desktop-wallpaper-lion-face-iphone-lion-art.jpg to (360, 640)\n",
            "Processed: dharni-sahaja-yoga-miracle-photo-krishna-puja-19-1024x768.jpg to (640, 480)\n",
            "Processed: dolomiti-italy-autumn-lago-antorno-landscape-photography-desktop-hd-wallpaper-for-pc-tablet-and-mobile-3840×2400-wallpaper-thumb.jpg to (496, 310)\n",
            "Processed: dusk.jpeg to (360, 640)\n",
            "Processed: e1033993550cfa0ccac63c0a25473c3f.jpg to (360, 640)\n",
            "Processed: earth.jpeg to (294, 640)\n",
            "Processed: edf38cf85c785a3f119e9f640cad61f4.jpg to (360, 640)\n",
            "Processed: Esc.jpeg to (295, 640)\n",
            "Processed: F1.jpeg to (360, 640)\n",
            "Processed: f8da5c50-13e3-4b76-bb39-64d5eae10aa5_1000x600.jpg to (640, 384)\n",
            "Processed: film_300_1600x900_gallery_1-yes.jpg to (640, 360)\n",
            "Processed: fix.jpeg to (295, 640)\n",
            "Processed: Flowers.jpeg to (273, 640)\n",
            "Processed: focus-black-bz.jpg to (640, 415)\n",
            "Processed: forest-6364913_640.jpg to (640, 427)\n",
            "Processed: forest-7150274_1280.jpg to (640, 426)\n",
            "Processed: forulas.jpeg to (360, 640)\n",
            "Processed: free-photo-of-stargazer-beneath-the-milky-way.jpeg to (419, 640)\n",
            "Processed: fuji-mountain-with-milky-way-night.jpg to (639, 360)\n",
            "Processed: G0qtjntXUAAW_jJ.jpeg to (288, 640)\n",
            "Processed: G8C61XoaMAAEPR2.jpg to (288, 640)\n",
            "Processed: G8dzS3Ra8AA-Y79.jpg to (320, 640)\n",
            "Processed: G8hrZdhaQAAvHBd.jpg to (320, 640)\n",
            "Processed: G8hsl93akAARpUn.jpg to (295, 640)\n",
            "Processed: G8Y2mx3bwAEHyj8.jpg to (320, 640)\n",
            "Processed: G9AusbmbIAAb4X2.jpg to (295, 640)\n",
            "Processed: G9AvTgkb0AIa9ZD.jpg to (295, 640)\n",
            "Processed: G9O8BGXakAAMqfH.jpg to (306, 640)\n",
            "Processed: galaxy-nature-aesthetic-background-starry-sky-mountain-remixed-media.jpg to (640, 426)\n",
            "Processed: Gemini_Generated_Image_j4hcuqj4hcuqj4hc.png to (640, 640)\n",
            "Processed: Gemini_Generated_Image_kul5kvkul5kvkul5.png to (640, 640)\n",
            "Processed: Gemini_Generated_Image_wijckvwijckvwijc.png to (640, 640)\n",
            "Processed: Gemini_Generated_Image_xdv5pbxdv5pbxdv5.png to (640, 640)\n",
            "Processed: Gemini_Generated_Image_xj2rdxxj2rdxxj2r.png to (640, 640)\n",
            "Processed: GQrGCNWWMAAGeBZ.jpg to (497, 640)\n",
            "Processed: grand-canyon-usa-85528-2.jpg to (640, 426)\n",
            "Processed: Greek AI.jpeg to (320, 640)\n",
            "Processed: green-bench-in-park-p6emimadkuw47fw0.jpg to (640, 360)\n",
            "Processed: Hands.jpg to (640, 384)\n",
            "Processed: hd-lion-onca-pintada-animals-big-cats-wallpaper-preview.jpg to (640, 511)\n",
            "Processed: HD-wallpaper-best-walpaper-art-artwork-bird-artificial-stars-original-phone-wall-night.jpg to (360, 640)\n",
            "Processed: HD-wallpaper-black-colour-ka-wink-emoji-black-background-thumbnail.jpg to (338, 601)\n",
            "Processed: HD-wallpaper-black-lamborghini-murcielago-racing-murcielago-racing-black-lamborghini.jpg to (640, 400)\n",
            "Processed: HD-wallpaper-dark-winter-lake-sunlight-winter-lake-sunlight-nature.jpg to (640, 360)\n",
            "Processed: HD-wallpaper-fondo-oscuro-planetas-soledad-universo-universo-oscuro.jpg to (360, 640)\n",
            "Processed: HD-wallpaper-imam-ali-as-iraq-islam-muslim-najaf-shia-shrine-sky.jpg to (640, 568)\n",
            "Processed: HD-wallpaper-karbala-imam-hussain-islam-love-shia-thumbnail.jpg to (338, 601)\n",
            "Processed: HD-wallpaper-midnight-calm-stars-dark-peaceful-beautiful-reflections-sky-night.jpg to (640, 413)\n",
            "Processed: HD-wallpaper-moody-blue-night-mountains-beauty-clouds-lake-blue.jpg to (640, 480)\n",
            "Processed: HD-wallpaper-real-dream-anime-best-cloud-cosmos-earth-land-legendary-nature-world.jpg to (360, 640)\n",
            "Processed: horse with wings.png to (640, 640)\n",
            "Processed: I'm fine.jpeg to (360, 640)\n",
            "Processed: IgUAa0.jpg to (640, 400)\n",
            "Processed: Imam Ali 2.jpg to (426, 640)\n",
            "Processed: istockphoto-508039012-612x612.jpg to (408, 612)\n",
            "Processed: kailah_3.jpg to (612, 406)\n",
            "Processed: Kailash.jpg to (640, 360)\n",
            "Processed: kailash_2.jpg to (640, 238)\n",
            "Processed: kailash_3.jpg to (540, 360)\n",
            "Processed: KnowThySelf-830x553.jpeg to (640, 426)\n",
            "Processed: la cheistrt.jpeg to (426, 640)\n",
            "Processed: la.jpeg to (358, 640)\n",
            "Processed: landscape-sunset-dark-nature-wallpaper-preview.jpg to (640, 426)\n",
            "Processed: Landscape-Wallpapers-Full-HD.jpg to (640, 360)\n",
            "Processed: leopard-animal-roar-muljjjvgfxgeew12.jpg to (640, 400)\n",
            "Processed: lighting-3489394_1280.jpg to (426, 640)\n",
            "Processed: Lion-Looking-Sky-Animal-Nature-Dark-Photo-iphone-8.jpg to (359, 640)\n",
            "Processed: lion-wild-african-predator-black-background-3840x2160-1534.jpg to (640, 360)\n",
            "Processed: Lion.jpeg to (294, 640)\n",
            "Processed: loading-bar-75-percentage-galaxy-note-hd-wallpaper.jpg to (500, 640)\n",
            "Processed: Lotus Flower.jpg to (450, 262)\n",
            "Processed: Lotus HD.jpg to (350, 567)\n",
            "Processed: lotus-on-blue-1800w.jpg to (450, 291)\n",
            "Processed: main-qimg-60f0273db5679455def324c5f9245c67.jpg to (602, 258)\n",
            "Processed: man space.jpeg to (320, 640)\n",
            "Processed: mataji_image1.jpg to (420, 640)\n",
            "Processed: matte-black-lamborghini-dark-28-09-2024-1727586576-hd-wallpaper.jpg to (360, 640)\n",
            "Processed: meditation_chakra_aura_122086_1920x1080.jpg to (640, 360)\n",
            "Processed: Minimalist-Dark-Wallpaper-1920x1080-65049.jpg to (640, 360)\n",
            "Processed: Mountain.jpeg to (323, 640)\n",
            "Processed: mountains-landscape-dark-nature-4k_1551643707.jpg to (640, 360)\n",
            "Processed: nature 2.jpeg to (288, 640)\n",
            "Processed: nature shaedes.jpeg to (288, 640)\n",
            "Processed: nature-background-high-resolution-wallpaper-for-a-serene-and-stunning-view-photo.jpg to (625, 350)\n",
            "Processed: nature.jpeg to (359, 640)\n",
            "Processed: Night lotus.jpg to (340, 244)\n",
            "Processed: ocean colors.jpg to (540, 360)\n",
            "Processed: ok not to be ok.jpeg to (288, 640)\n",
            "Processed: old phone.jpeg to (295, 640)\n",
            "Processed: orad.jpeg to (295, 640)\n",
            "Processed: papers.co-ml14-sunset-lake-night-dark-nature-35-3840x2160-4k-wallpaper.jpg to (640, 360)\n",
            "Processed: parrot-3601194_640.jpg to (640, 427)\n",
            "Processed: path_in_dark_forest_4k_hd_dark-3840x2160.jpg to (640, 360)\n",
            "Processed: peace.jpeg to (295, 640)\n",
            "Processed: pexels-aadil-2884867.jpg to (427, 640)\n",
            "Processed: pexels-adrien-olichon-1257089-2823459.jpg to (426, 640)\n",
            "Processed: pexels-alex-andrews-271121-816608.jpg to (408, 640)\n",
            "Processed: pexels-belliragon-6425781.jpg to (480, 640)\n",
            "Processed: pexels-carlos-oliva-1966452-3586966.jpg to (640, 426)\n",
            "Processed: pexels-chaitaastic-2666598.jpg to (640, 360)\n",
            "Processed: pexels-eberhardgross-1624496.jpg to (426, 640)\n",
            "Processed: pexels-eberhardgross-2098427.jpg to (426, 640)\n",
            "Processed: pexels-eberhardgross-691668.jpg to (640, 426)\n",
            "Processed: pexels-esan-2085998.jpg to (640, 426)\n",
            "Processed: pexels-fabiano-rodrigues-794857-1662298.jpg to (640, 426)\n",
            "Processed: pexels-felix-mittermeier-2832084.jpg to (640, 426)\n",
            "Processed: pexels-iriser-1379636.jpg to (640, 426)\n",
            "Processed: pexels-iriser-1379640.jpg to (640, 426)\n",
            "Processed: pexels-johannes-plenio-1102912.jpg to (640, 426)\n",
            "Processed: pexels-jovana-nesic-188639-593655.jpg to (640, 426)\n",
            "Processed: pexels-jplenio-3262249.jpg to (640, 360)\n",
            "Processed: pexels-leventsimsek-3689269.jpg to (426, 640)\n",
            "Processed: pexels-lil-artsy-1213447.jpg to (640, 426)\n",
            "Processed: pexels-marieke-schonfeld-1309710-2514035.jpg to (640, 426)\n",
            "Processed: pexels-markp-2790391.jpg to (640, 426)\n",
            "Processed: pexels-mitchel3uo-3694706.jpg to (426, 640)\n",
            "Processed: pexels-photo-1213447.jpeg to (500, 333)\n",
            "Processed: pexels-photo-799443 (1).jpeg to (426, 640)\n",
            "Processed: pexels-photo-799443.jpeg to (426, 640)\n",
            "Processed: pexels-picturemechaniq-1749303.jpg to (640, 426)\n",
            "Processed: pexels-pierre-blache-651604-2834219.jpg to (512, 639)\n",
            "Processed: pexels-pixabay-33545.jpg to (640, 422)\n",
            "Processed: pexels-sanaan-3052361 (1).jpg to (424, 640)\n",
            "Processed: pexels-sanaan-3052361.jpg to (424, 640)\n",
            "Processed: pexels-sebastiaan9977-1097456.jpg to (640, 426)\n",
            "Processed: pexels-sebastiaan9977-1480690.jpg to (640, 426)\n",
            "Processed: pexels-semws-2670898 (1).jpg to (426, 640)\n",
            "Processed: pexels-semws-2670898.jpg to (426, 640)\n",
            "Processed: pexels-sohi-807598.jpg to (640, 480)\n",
            "Processed: pexels-steve-1269968.jpg to (640, 403)\n",
            "Processed: pexels-surender-singh-561453-1317013.jpg to (640, 426)\n",
            "Processed: photo-1477132394330-d2376dc4c091.jpeg to (640, 367)\n",
            "Processed: photo-1499419865879-453926ae8a72.jpeg to (426, 640)\n",
            "Processed: photo-1605854770838-8e5b4d6beaac.jpeg to (426, 640)\n",
            "Processed: Pi.jpeg to (288, 640)\n",
            "Processed: Plain-Black-Wallpaper-8.jpg to (295, 640)\n",
            "Processed: plane in city.jpeg to (295, 640)\n",
            "Processed: Planet earth.jpeg to (365, 640)\n",
            "Processed: planets.jpeg to (294, 640)\n",
            "Processed: pngtree-abstract-background-with-dots-and-lines-connecting-generated-by-4k-rendering-technology-photo-image_25044558.jpg to (640, 360)\n",
            "Processed: pngtree-new-art-1080p-2k-4k-5k-hd-wallpapers-free-download-background-image_16123072.jpg to (390, 260)\n",
            "Processed: premium_photo-1682621097202-eca012076ff2.jpeg to (426, 640)\n",
            "Processed: premium_photo-1712029680467-945b17d26627.jpeg to (640, 426)\n",
            "Processed: premium_photo-1721276303391-ee0af231d021.jpg to (640, 360)\n",
            "Processed: quote-from-abraham-lincoln-with-deep-psychological-meaning-hnz1ov7gj4dsf31q.jpg to (640, 360)\n",
            "Processed: rag.png to (640, 365)\n",
            "Processed: ragnar-lodbrok-digital-art-vikings-sword-wallpaper-preview.jpg to (640, 360)\n",
            "Processed: river-stream-dark-4k-ipad-pro.jpg to (640, 640)\n",
            "Processed: road.jpeg to (295, 640)\n",
            "Processed: shiva.jpg to (320, 640)\n",
            "Processed: slow-walker-never-walk-back-abraham-lincoln-popular-quotes-wallpaper-preview.jpg to (640, 360)\n",
            "Processed: sm-miracle-vibrations.jpg to (640, 433)\n",
            "Processed: sony-world-photography-awards-2015-shortlist-3.jpg to (640, 428)\n",
            "Processed: sony-world-photography-awards-2015-shortlist-6.jpg to (640, 424)\n",
            "Processed: sony-world-photography-awards-2019-121-5c619634b28a0__880.jpg to (640, 427)\n",
            "Processed: space.jpeg to (294, 640)\n",
            "Processed: stay-humble-hustle-hard-thumbnail.jpg to (350, 350)\n",
            "Processed: suurender your thought to life.jpg to (409, 273)\n",
            "Processed: The-cattle-herd-in-the-mist-min.jpg to (640, 426)\n",
            "Processed: the-city-fog-skyscrapers-chicago-wallpaper-preview.jpg to (640, 426)\n",
            "Processed: the-sky-sport-battle-wallpaper-preview.jpg to (640, 480)\n",
            "Processed: three pics.jpeg to (288, 640)\n",
            "Processed: tiger-amazing-background-hd-wallpaper-photo.jpeg to (625, 350)\n",
            "Processed: tiger-eyes-looking-from-the-bushes-free-image.jpeg to (374, 640)\n",
            "Processed: tree.jpeg to (329, 640)\n",
            "Processed: Tri.jpeg to (360, 640)\n",
            "Processed: Viegnt.jpg to (640, 359)\n",
            "Processed: wallpaper2you_147725.jpg to (640, 360)\n",
            "Processed: wallpaper2you_355149.jpg to (360, 640)\n",
            "Processed: wallpaper2you_458470.jpg to (640, 400)\n",
            "Processed: wallpapersden.com_artistic-night-8k_7680x4320.jpg to (640, 360)\n",
            "Processed: wallpapersden.com_k-dark-sunset_3840x2160.jpg to (640, 360)\n",
            "Processed: wallpapersden.com_synthwave-dark-minimal-art_3280x1800.jpg to (640, 351)\n",
            "Processed: wallpapersden.com_tree-alone-dark-evening-4k_3840x2160 (1).jpg to (640, 360)\n",
            "Processed: wallpapersden.com_tree-alone-dark-evening-4k_3840x2160.jpg to (640, 360)\n",
            "Processed: water.jpg to (168, 300)\n",
            "Processed: work hard stay humble.png to (640, 308)\n",
            "Processed: world.jpeg to (360, 640)\n",
            "Processed: wp12407134.jpg to (640, 360)\n",
            "Processed: wp14374892.jpg to (425, 640)\n",
            "Processed: wp2040400.jpg to (640, 360)\n",
            "Processed: wp2604675.jpg to (640, 400)\n",
            "Processed: wp3262706.jpg to (640, 360)\n",
            "Processed: wp4009177.jpg to (640, 400)\n",
            "Processed: wp4575076.jpg to (360, 640)\n",
            "Processed: wp6689718.jpg to (640, 360)\n",
            "Processed: WSLYqnXOVRrPpNkwdEQo.jpg to (358, 640)\n",
            "Processed: You are here.jpeg to (360, 640)\n",
            "Processed: لا اله الا الله.png to (640, 432)\n",
            "Processed: نجف.jpg to (426, 640)\n",
            "Processed: وما رميت اذ رميت.jpeg to (521, 640)\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "\n",
        "def resize_images(input_folder, output_folder, max_size=640): \n",
        "    if not os.path.exists(output_folder):\n",
        "        os.makedirs(output_folder)\n",
        "    \n",
        "    # Empty the output folder\n",
        "    for filename in os.listdir(output_folder):\n",
        "        file_path = os.path.join(output_folder, filename)\n",
        "        try:\n",
        "            if os.path.isfile(file_path) or os.path.islink(file_path):\n",
        "                os.unlink(file_path)\n",
        "            elif os.path.isdir(file_path):\n",
        "                shutil.rmtree(file_path)\n",
        "        except Exception as e:\n",
        "            print(f'Failed to delete {file_path}. Reason: {e}')\n",
        "\n",
        "    # Loop through all files in the input folder\n",
        "    for filename in os.listdir(input_folder):\n",
        "        if filename.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "            img_path = os.path.join(input_folder, filename)\n",
        "            \n",
        "            with Image.open(img_path) as img: \n",
        "                width, height = img.size \n",
        "                scale = max_size / float(max(width, height)) \n",
        "                if scale < 1.0:\n",
        "                    new_size = (int(width * scale), int(height * scale)) \n",
        "                    img = img.resize(new_size, Image.Resampling.LANCZOS)\n",
        "                \n",
        "                # Save to the output folder\n",
        "                img.save(os.path.join(output_folder, filename))\n",
        "                print(f\"Processed: {filename} to {img.size}\")\n",
        "        else:\n",
        "            #delete image \n",
        "            os.remove(os.path.join(input_folder, filename))\n",
        "\n",
        "\n",
        "# Usage\n",
        "input_dir = r'C:\\Users\\user\\OneDrive\\Pictures\\wallpapers'\n",
        "output_dir = r'C:\\Users\\user\\OneDrive\\Pictures\\resized_wallpapers'\n",
        "resize_images(input_dir, output_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "85510885",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4af0b4be",
      "metadata": {},
      "outputs": [],
      "source": [
        "# uv init\n",
        "# uv venv torch_env\n",
        "# uv venv torch_gpu --python=3.13\n",
        "# uv venv torch_gpu --python \"C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python313\\python.exe\"\n",
        "\n",
        "# torch_env\\Scripts\\activate\n",
        "# uv add matplotlib pycocotools opencv-python Pillow torch ultralytics rfdetr pyyaml ipykernel\n",
        "\n",
        "## Had to use pip, uv still have some issues with GPU \n",
        "# uv pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
        "# uv pip install ultralytics\n",
        "# uv pip install rfdetr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "37c0c0a3",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hello World\n"
          ]
        }
      ],
      "source": [
        "print(\"Hello World\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "91084cca",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "win32\n",
            "nt\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "print(sys.platform)\n",
        "print(os.name)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ed73a758",
      "metadata": {},
      "source": [
        "#### Single GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fec22d55",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running in notebook mode\n",
            "GPU is available. Proceeding with training on GPU.\n",
            "[GPU0] Epoch 0 | Steps: 128\n",
            "Epoch 0 | Training checkpoint saved at trained_models/checkpoint_epoch_0.pt\n",
            "[GPU0] Epoch 1 | Steps: 128\n",
            "[GPU0] Epoch 2 | Steps: 128\n",
            "Epoch 2 | Training checkpoint saved at trained_models/checkpoint_epoch_2.pt\n",
            "[GPU0] Epoch 3 | Steps: 128\n",
            "[GPU0] Epoch 4 | Steps: 128\n",
            "Epoch 4 | Training checkpoint saved at trained_models/checkpoint_epoch_4.pt\n",
            "[GPU0] Epoch 5 | Steps: 128\n",
            "[GPU0] Epoch 6 | Steps: 128\n",
            "Epoch 6 | Training checkpoint saved at trained_models/checkpoint_epoch_6.pt\n",
            "[GPU0] Epoch 7 | Steps: 128\n",
            "[GPU0] Epoch 8 | Steps: 128\n",
            "Epoch 8 | Training checkpoint saved at trained_models/checkpoint_epoch_8.pt\n",
            "[GPU0] Epoch 9 | Steps: 128\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from datautils import MyTrainDataset\n",
        "from utils import print_nvidia_smi\n",
        "\n",
        "\n",
        "class Trainer:\n",
        "    def __init__(\n",
        "        self,\n",
        "        model: torch.nn.Module,\n",
        "        train_dataloader: DataLoader,\n",
        "        optimizer: torch.optim.Optimizer,\n",
        "        gpu_id: int,\n",
        "        save_every: int,\n",
        "    ):\n",
        "        self.model = model.to(gpu_id)\n",
        "        self.train_dataloader = train_dataloader\n",
        "        self.optimizer = optimizer        \n",
        "        self.gpu_id = gpu_id\n",
        "        self.save_every = save_every\n",
        "\n",
        "    def _run_batch(self, source, targets):\n",
        "        self.optimizer.zero_grad()\n",
        "        output = self.model(source)\n",
        "        loss = F.cross_entropy(output, targets)\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "    def _run_epoch(self, epoch):\n",
        "        print(f\"[GPU{self.gpu_id}] Epoch {epoch} | Steps: {len(self.train_dataloader)}\")\n",
        "        for source, targets in self.train_dataloader:\n",
        "            source = source.to(self.gpu_id)\n",
        "            targets = targets.to(self.gpu_id)\n",
        "            self._run_batch(source, targets)\n",
        "\n",
        "    def _save_checkpoint(self, epoch):\n",
        "        ckp = self.model.state_dict()\n",
        "        if not os.path.exists(\"trained_models\"):\n",
        "            os.makedirs(\"trained_models\")\n",
        "        PATH = f\"trained_models/checkpoint_epoch_{epoch}.pt\"\n",
        "        torch.save(ckp, PATH)\n",
        "        print(f\"Epoch {epoch} | Training checkpoint saved at {PATH}\")\n",
        "\n",
        "    def train(self, total_epochs: int):\n",
        "        for epoch in range(total_epochs):\n",
        "            self._run_epoch(epoch)\n",
        "            if epoch % self.save_every == 0:\n",
        "                self._save_checkpoint(epoch)\n",
        "\n",
        "\n",
        "def load_train_objs():\n",
        "    train_dataset = MyTrainDataset(2048)\n",
        "    model = torch.nn.Linear(20, 1)\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
        "    return train_dataset, model, optimizer\n",
        "\n",
        "\n",
        "def prepare_dataloader(dataset: Dataset, batch_size: int):\n",
        "    return DataLoader(dataset, batch_size=batch_size, pin_memory=True, shuffle=True)\n",
        "\n",
        "\n",
        "def main(device, total_epochs, batch_size, save_every):\n",
        "    # Check if GPU is available\n",
        "    if not torch.cuda.is_available():\n",
        "        raise RuntimeError(\"GPU is not available. This script requires a GPU to run.\")\n",
        "    else:\n",
        "        print(\"GPU is available. Proceeding with training on GPU.\")\n",
        "\n",
        "    dataset, model, optimizer = load_train_objs()\n",
        "    train_dataloader = prepare_dataloader(dataset, batch_size)\n",
        "    trainer = Trainer(model, train_dataloader, optimizer, device, save_every)\n",
        "    trainer.train(total_epochs)\n",
        "\n",
        "\n",
        "# Check if running in a notebook\n",
        "def is_notebook():\n",
        "    try:\n",
        "        shell = get_ipython().__class__.__name__\n",
        "        if shell == 'ZMQInteractiveShell':\n",
        "            return True   # Jupyter notebook or qtconsole\n",
        "        elif 'google.colab' in str(get_ipython()):\n",
        "            return True   # Google Colab\n",
        "        else:\n",
        "            return False\n",
        "    except NameError:\n",
        "        return False      # Probably standard Python interpreter\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    if is_notebook():\n",
        "        # Notebook mode: set parameters directly\n",
        "        print(\"Running in notebook mode\")\n",
        "        total_epochs = 10\n",
        "        batch_size = 16\n",
        "        save_every = 2\n",
        "    else:\n",
        "        # Script mode: use argparse\n",
        "        import argparse\n",
        "        parser = argparse.ArgumentParser(description='simple distributed training job')\n",
        "        parser.add_argument('--total_epochs', type=int, default=1, help='Total epochs to train the model')    \n",
        "        parser.add_argument('--batch_size', type=int, default=32, help='Input batch size on each device (default: 32)')\n",
        "        parser.add_argument('--save_every', type=int, default=1, help='How often to save a snapshot')\n",
        "        args = parser.parse_args()\n",
        "        \n",
        "        total_epochs = args.total_epochs\n",
        "        batch_size = args.batch_size\n",
        "        save_every = args.save_every\n",
        "\n",
        "    # print_nvidia_smi()\n",
        "    device = 0  # shorthand for cuda:0\n",
        "    main(device, total_epochs, batch_size, save_every) \n",
        "\n",
        "## Usage:\n",
        "# In notebook: Just run the cell, modify the parameters in the notebook mode section\n",
        "# As script: python 1_single_gpu.py --total_epochs 10 --batch_size 64 --save_every 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6438b3b9",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "f0b006ba",
      "metadata": {},
      "source": [
        "#### Multi-GPUs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "905587c3",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running in notebook mode\n",
            "Warning: Only 1 GPU(s) detected. DDP is designed for multi-GPU training.\n",
            "Running DDP anyway for testing purposes...\n",
            "\n"
          ]
        },
        {
          "ename": "ProcessExitedException",
          "evalue": "process 0 terminated with exit code 1",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mProcessExitedException\u001b[39m                    Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 145\u001b[39m\n\u001b[32m    143\u001b[39m     \u001b[38;5;66;03m# FIXED: Correct argument order matching function signature\u001b[39;00m\n\u001b[32m    144\u001b[39m     spawn_arguments = (world_size, total_epochs, save_every, batch_size)\n\u001b[32m--> \u001b[39m\u001b[32m145\u001b[39m     \u001b[43mmp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mspawn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mspawn_arguments\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnprocs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mworld_size\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m    147\u001b[39m \u001b[38;5;66;03m## Usage example:\u001b[39;00m\n\u001b[32m    148\u001b[39m \u001b[38;5;66;03m# python 2_multigpu.py --total_epochs 10 --batch_size 64 --save_every 2\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\Desktop\\My_Files\\Anaconda_Projects\\Projects\\distributed-training\\.venv\\Lib\\site-packages\\torch\\multiprocessing\\spawn.py:364\u001b[39m, in \u001b[36mspawn\u001b[39m\u001b[34m(fn, args, nprocs, join, daemon, start_method)\u001b[39m\n\u001b[32m    358\u001b[39m     msg = (\n\u001b[32m    359\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mThis method only supports start_method=spawn (got: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstart_method\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m).\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    360\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mTo use a different start_method use:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    361\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m torch.multiprocessing.start_processes(...)\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    362\u001b[39m     )\n\u001b[32m    363\u001b[39m     warnings.warn(msg, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel=\u001b[32m2\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m364\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mstart_processes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnprocs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdaemon\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_method\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mspawn\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\Desktop\\My_Files\\Anaconda_Projects\\Projects\\distributed-training\\.venv\\Lib\\site-packages\\torch\\multiprocessing\\spawn.py:320\u001b[39m, in \u001b[36mstart_processes\u001b[39m\u001b[34m(fn, args, nprocs, join, daemon, start_method, numa_options)\u001b[39m\n\u001b[32m    317\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m context\n\u001b[32m    319\u001b[39m \u001b[38;5;66;03m# Loop on join until it returns True or raises an exception.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m320\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m    321\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\Desktop\\My_Files\\Anaconda_Projects\\Projects\\distributed-training\\.venv\\Lib\\site-packages\\torch\\multiprocessing\\spawn.py:209\u001b[39m, in \u001b[36mProcessContext.join\u001b[39m\u001b[34m(self, timeout, grace_period)\u001b[39m\n\u001b[32m    201\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m ProcessExitedException(\n\u001b[32m    202\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mprocess \u001b[39m\u001b[38;5;132;01m{\u001b[39;00merror_index\u001b[38;5;132;01m:\u001b[39;00m\u001b[33md\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m terminated with signal \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    203\u001b[39m             error_index=error_index,\n\u001b[32m   (...)\u001b[39m\u001b[32m    206\u001b[39m             signal_name=name,\n\u001b[32m    207\u001b[39m         )\n\u001b[32m    208\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m209\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m ProcessExitedException(\n\u001b[32m    210\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mprocess \u001b[39m\u001b[38;5;132;01m{\u001b[39;00merror_index\u001b[38;5;132;01m:\u001b[39;00m\u001b[33md\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m terminated with exit code \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexitcode\u001b[38;5;132;01m:\u001b[39;00m\u001b[33md\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    211\u001b[39m             error_index=error_index,\n\u001b[32m    212\u001b[39m             error_pid=failed_process.pid,\n\u001b[32m    213\u001b[39m             exit_code=exitcode,\n\u001b[32m    214\u001b[39m         )\n\u001b[32m    216\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;28mself\u001b[39m.error_files[error_index], \u001b[33m\"\u001b[39m\u001b[33mrb\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m fh:\n\u001b[32m    217\u001b[39m     original_trace = pickle.load(fh)\n",
            "\u001b[31mProcessExitedException\u001b[39m: process 0 terminated with exit code 1"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from datautils import MyTrainDataset\n",
        "\n",
        "import torch.multiprocessing as mp\n",
        "from torch.utils.data.distributed import DistributedSampler\n",
        "from torch.nn.parallel import DistributedDataParallel as DDP\n",
        "from torch.distributed import init_process_group, destroy_process_group\n",
        "import os\n",
        "\n",
        "\n",
        "def ddp_setup(rank, world_size):\n",
        "    os.environ[\"MASTER_ADDR\"] = \"localhost\"\n",
        "    os.environ[\"MASTER_PORT\"] = \"12355\"\n",
        "    torch.cuda.set_device(rank)\n",
        "    \n",
        "    # Use nccl for Linux/Mac, gloo for Windows\n",
        "    backend = \"gloo\" if os.name == \"nt\" else \"nccl\"\n",
        "    init_process_group(backend=backend, rank=rank, world_size=world_size)\n",
        "\n",
        "\n",
        "class Trainer:\n",
        "    def __init__(\n",
        "        self,\n",
        "        model: torch.nn.Module,\n",
        "        train_data: DataLoader,\n",
        "        optimizer: torch.optim.Optimizer,\n",
        "        gpu_id: int,\n",
        "        save_every: int,\n",
        "    ) -> None:\n",
        "        self.gpu_id = gpu_id\n",
        "        self.model = model.to(gpu_id)\n",
        "        self.train_data = train_data\n",
        "        self.optimizer = optimizer\n",
        "        self.save_every = save_every\n",
        "        \n",
        "        self.model = DDP(self.model, device_ids=[gpu_id])\n",
        "\n",
        "    def _run_batch(self, source, targets):\n",
        "        self.optimizer.zero_grad()\n",
        "        output = self.model(source)\n",
        "        loss = F.cross_entropy(output, targets)\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "    def _run_epoch(self, epoch):\n",
        "        self.train_data.sampler.set_epoch(epoch)  # type: ignore\n",
        "        print(f\"[GPU{self.gpu_id}] Epoch {epoch} | Steps: {len(self.train_data)}\")\n",
        "        for source, targets in self.train_data:\n",
        "            source = source.to(self.gpu_id)\n",
        "            targets = targets.to(self.gpu_id)\n",
        "            self._run_batch(source, targets)\n",
        "\n",
        "    def _save_checkpoint(self, epoch):\n",
        "        ckp = self.model.module.state_dict()  # type: ignore\n",
        "        if not os.path.exists(\"trained_models\"):\n",
        "            os.makedirs(\"trained_models\")\n",
        "        PATH = f\"trained_models/checkpoint_epoch_{epoch}.pt\"\n",
        "        torch.save(ckp, PATH)\n",
        "        print(f\"Epoch {epoch} | Training checkpoint saved at {PATH}\")\n",
        "\n",
        "    def train(self, max_epochs: int):\n",
        "        for epoch in range(max_epochs):\n",
        "            self._run_epoch(epoch)\n",
        "            if self.gpu_id == 0 and epoch % self.save_every == 0:\n",
        "                self._save_checkpoint(epoch)\n",
        "\n",
        "\n",
        "def load_train_objs():\n",
        "    train_set = MyTrainDataset(2048)  # load your dataset\n",
        "    model = torch.nn.Linear(20, 1)  # load your model\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
        "    return train_set, model, optimizer\n",
        "\n",
        "\n",
        "def prepare_dataloader(dataset: Dataset, batch_size: int):\n",
        "    return DataLoader(\n",
        "        dataset,\n",
        "        batch_size=batch_size,\n",
        "        pin_memory=True,\n",
        "        shuffle=False,\n",
        "        sampler=DistributedSampler(dataset)\n",
        "    )\n",
        "\n",
        "\n",
        "def main(rank: int, world_size: int, total_epochs: int, save_every: int, batch_size: int):\n",
        "    ddp_setup(rank, world_size)\n",
        "    dataset, model, optimizer = load_train_objs()\n",
        "    train_data = prepare_dataloader(dataset, batch_size)\n",
        "    trainer = Trainer(model, train_data, optimizer, rank, save_every)\n",
        "    trainer.train(total_epochs)\n",
        "    destroy_process_group()\n",
        "\n",
        "\n",
        "# Check if running in a notebook\n",
        "def is_notebook():\n",
        "    try:\n",
        "        shell = get_ipython().__class__.__name__\n",
        "        if shell == 'ZMQInteractiveShell':\n",
        "            return True   # Jupyter notebook or qtconsole\n",
        "        elif 'google.colab' in str(get_ipython()):\n",
        "            return True   # Google Colab\n",
        "        else:\n",
        "            return False\n",
        "    except NameError:\n",
        "        return False      # Probably standard Python interpreter\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    if is_notebook():\n",
        "        # Notebook mode: set parameters directly\n",
        "        print(\"Running in notebook mode\")\n",
        "        total_epochs = 10\n",
        "        batch_size = 32\n",
        "        save_every = 1\n",
        "    else:\n",
        "        # Script mode: use argparse\n",
        "        import argparse\n",
        "        \n",
        "        parser = argparse.ArgumentParser(description='simple distributed training job')\n",
        "        parser.add_argument('--total_epochs', type=int, default=10, help='Total epochs to train the model')    \n",
        "        parser.add_argument('--batch_size', type=int, default=32, help='Input batch size on each device (default: 32)')\n",
        "        parser.add_argument('--save_every', type=int, default=1, help='How often to save a snapshot')\n",
        "        args = parser.parse_args()\n",
        "        \n",
        "        total_epochs = args.total_epochs\n",
        "        batch_size = args.batch_size\n",
        "        save_every = args.save_every\n",
        "\n",
        "    world_size = torch.cuda.device_count()\n",
        "    \n",
        "    if world_size < 2:\n",
        "        print(f\"Warning: Only {world_size} GPU(s) detected. DDP is designed for multi-GPU training.\")\n",
        "        print(\"Running DDP anyway for testing purposes...\\n\")\n",
        "    else:\n",
        "        print(f\"Detected {world_size} GPUs. Using DDP multi-GPU training mode.\\n\")\n",
        "    \n",
        "    # Set spawn method for Windows (required for multiprocessing)\n",
        "    if os.name == \"nt\":\n",
        "        mp.set_start_method('spawn', force=True)\n",
        "    \n",
        "    # FIXED: Correct argument order matching function signature\n",
        "    spawn_arguments = (world_size, total_epochs, save_every, batch_size)\n",
        "    mp.spawn(main, args=spawn_arguments, nprocs=world_size)  # type: ignore\n",
        "\n",
        "## Usage example:\n",
        "# python 2_multigpu.py --total_epochs 10 --batch_size 64 --save_every 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "30f09bcf",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6eb6c95d",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "456aacfc",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ee3294a2",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ace3172c",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "938aa448",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f73a1724",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "dd9b8ef0-5d69-44b7-8da9-10ec7306dd4d",
      "metadata": {
        "id": "dd9b8ef0-5d69-44b7-8da9-10ec7306dd4d"
      },
      "source": [
        "# DataParallel Notebook\n",
        "-------------------------\n",
        "Inspired by: [Pytorch Data Parallelism Tutorial](https://docs.pytorch.org/tutorials/beginner/blitz/data_parallel_tutorial.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "34c98ebd",
      "metadata": {
        "id": "34c98ebd",
        "lines_to_next_cell": 2
      },
      "source": [
        "# Imports and Initial Setup\n",
        "------------------------------------\n",
        "This cell imports the necessary PyTorch libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6db1e4d5",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-06-18T14:30:01.357274Z",
          "iopub.status.busy": "2025-06-18T14:30:01.356521Z",
          "iopub.status.idle": "2025-06-18T14:30:01.360775Z",
          "shell.execute_reply": "2025-06-18T14:30:01.359986Z",
          "shell.execute_reply.started": "2025-06-18T14:30:01.357249Z"
        },
        "id": "6db1e4d5",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "print(f\"PyTorch Version: {torch.__version__}\")\n",
        "print(\"-\" * 30)\n",
        "\n",
        "# Check for available GPUs\n",
        "if torch.cuda.is_available():\n",
        "    num_gpus = torch.cuda.device_count()\n",
        "    print(f\"Found {num_gpus} GPUs.\")\n",
        "    # Set the primary device\n",
        "    device = torch.device(\"cuda:0\")\n",
        "else:\n",
        "    num_gpus = 0\n",
        "    print(\"No GPUs found. Running on CPU.\")\n",
        "    device = torch.device(\"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d6af9aca",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-06-18T14:30:21.610011Z",
          "iopub.status.busy": "2025-06-18T14:30:21.609490Z",
          "iopub.status.idle": "2025-06-18T14:30:21.615075Z",
          "shell.execute_reply": "2025-06-18T14:30:21.614246Z",
          "shell.execute_reply.started": "2025-06-18T14:30:21.609987Z"
        },
        "id": "d6af9aca",
        "lines_to_next_cell": 1,
        "outputId": "e0ebb557-e0c2-461c-fc16-169c04cb816d",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PyTorch Version: 2.9.0+cu128\n",
            "------------------------------\n",
            "Found 1 GPUs.\n"
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "0c00bc52-470a-4526-826e-2216ff467778",
      "metadata": {
        "id": "0c00bc52-470a-4526-826e-2216ff467778"
      },
      "source": [
        "# Data Parallel\n",
        "-----------------------------\n",
        "Source: [DataParallel vs. DistributedDataParallel in PyTorch: What’s the Difference?](https://medium.com/@mlshark/dataparallel-vs-distributeddataparallel-in-pytorch-whats-the-difference-0af10bb43bc7)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b7715eb1",
      "metadata": {
        "id": "b7715eb1"
      },
      "source": [
        "# Define a Simple Model\n",
        "-----------------------------\n",
        "We'll create a basic neural network for this demonstration.\n",
        "DataParallel will replicate this model on each available GPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "137aa2ce",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-06-18T14:33:51.487053Z",
          "iopub.status.busy": "2025-06-18T14:33:51.486118Z",
          "iopub.status.idle": "2025-06-18T14:33:51.491916Z",
          "shell.execute_reply": "2025-06-18T14:33:51.491377Z",
          "shell.execute_reply.started": "2025-06-18T14:33:51.487021Z"
        },
        "id": "137aa2ce",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "class SimpleModel(nn.Module):\n",
        "    def __init__(self, input_size, output_size):\n",
        "        super(SimpleModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, 128)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(128, output_size)\n",
        "\n",
        "    def forward(self, x, debug=False):\n",
        "        out = self.fc1(x)\n",
        "        out = self.relu(out)\n",
        "        out = self.fc2(out)\n",
        "        if debug:\n",
        "            print(\"\\tInside the Model: input size\", x.size(), \"output size\", out.size())\n",
        "        return out\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "66d4b247",
      "metadata": {
        "id": "66d4b247",
        "lines_to_next_cell": 2
      },
      "source": [
        "# Data Preparation and Training Loop\n",
        "------------------------------------------\n",
        "This is the main part where we wrap our model with DataParallel\n",
        "and run the training process."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "84a4dd7e",
      "metadata": {
        "id": "84a4dd7e",
        "lines_to_next_cell": 0
      },
      "source": [
        "## 1. Hyperparameters and Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "113e6748",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-06-18T14:35:14.851088Z",
          "iopub.status.busy": "2025-06-18T14:35:14.850315Z",
          "iopub.status.idle": "2025-06-18T14:35:14.924285Z",
          "shell.execute_reply": "2025-06-18T14:35:14.923505Z",
          "shell.execute_reply.started": "2025-06-18T14:35:14.851066Z"
        },
        "id": "113e6748",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "input_size = 784\n",
        "output_size = 10\n",
        "batch_size = 256  # A larger batch size helps utilize multiple GPUs\n",
        "learning_rate = 0.01\n",
        "num_epochs = 20\n",
        "\n",
        "# Create dummy data\n",
        "# We create a dataset of 10000 samples\n",
        "inputs = torch.randn(10000, input_size)\n",
        "targets = torch.randint(0, output_size, (10000,))\n",
        "\n",
        "# Use DataLoader for batching\n",
        "dataset = TensorDataset(inputs, targets)\n",
        "# The batch size will be split across GPUs. If you have 2 GPUs,\n",
        "# each will process batch_size / 2 samples.\n",
        "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "23f49f33",
      "metadata": {
        "id": "23f49f33",
        "lines_to_next_cell": 0
      },
      "source": [
        "## 2. Initialize and Wrap the Model\n",
        "Instantiate the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "86c5fc36",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-06-18T14:36:20.905451Z",
          "iopub.status.busy": "2025-06-18T14:36:20.904773Z",
          "iopub.status.idle": "2025-06-18T14:36:20.913077Z",
          "shell.execute_reply": "2025-06-18T14:36:20.912464Z",
          "shell.execute_reply.started": "2025-06-18T14:36:20.905423Z"
        },
        "id": "86c5fc36",
        "outputId": "4ac456d3-b301-40da-f473-246346a973f9",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training on a single device (CPU or 1 GPU).\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "SimpleModel(\n",
              "  (fc1): Linear(in_features=784, out_features=128, bias=True)\n",
              "  (relu): ReLU()\n",
              "  (fc2): Linear(in_features=128, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = SimpleModel(input_size, output_size)\n",
        "\n",
        "# IMPORTANT: Wrap the model with nn.DataParallel\n",
        "# This is the key step for data parallelism.\n",
        "# If multiple GPUs are available, this wrapper will handle the data distribution.\n",
        "if num_gpus > 1:\n",
        "    print(f\"Using {num_gpus} GPUs for training!\")\n",
        "    device_ids = list(range(num_gpus))  # Explicitly specify all available GPUs\n",
        "    model = nn.DataParallel(model, device_ids=device_ids)\n",
        "else:\n",
        "    print(\"Training on a single device (CPU or 1 GPU).\")\n",
        "\n",
        "# Move the model to the primary device. DataParallel will handle the rest.\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "577fc2b9",
      "metadata": {
        "id": "577fc2b9",
        "lines_to_next_cell": 0
      },
      "source": [
        "## 3. Loss and Optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "95b25c7b",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-06-18T14:36:30.917424Z",
          "iopub.status.busy": "2025-06-18T14:36:30.917136Z",
          "iopub.status.idle": "2025-06-18T14:36:30.921335Z",
          "shell.execute_reply": "2025-06-18T14:36:30.920770Z",
          "shell.execute_reply.started": "2025-06-18T14:36:30.917405Z"
        },
        "id": "95b25c7b",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5ceddd76",
      "metadata": {
        "id": "5ceddd76",
        "lines_to_next_cell": 0
      },
      "source": [
        "## 4. Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "96086fd4",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-06-18T14:37:56.528430Z",
          "iopub.status.busy": "2025-06-18T14:37:56.527863Z",
          "iopub.status.idle": "2025-06-18T14:38:00.899463Z",
          "shell.execute_reply": "2025-06-18T14:38:00.898449Z",
          "shell.execute_reply.started": "2025-06-18T14:37:56.528408Z"
        },
        "id": "96086fd4",
        "outputId": "ec2d2032-7f21-46ee-c7ab-c913831b14e3",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Starting training...\n",
            "\tInside the Model: input size torch.Size([256, 784]) output size torch.Size([256, 10])\n",
            "Outside: input size torch.Size([256, 784]) output_size torch.Size([256, 10])\n",
            "Epoch [1/20], Loss: 2.3264\n",
            "Epoch [2/20], Loss: 2.3170\n",
            "Epoch [3/20], Loss: 2.3090\n",
            "Epoch [4/20], Loss: 2.3047\n",
            "Epoch [5/20], Loss: 2.2997\n",
            "Epoch [6/20], Loss: 2.2965\n",
            "Epoch [7/20], Loss: 2.2870\n",
            "Epoch [8/20], Loss: 2.2838\n",
            "Epoch [9/20], Loss: 2.2795\n",
            "Epoch [10/20], Loss: 2.2735\n",
            "Epoch [11/20], Loss: 2.2701\n",
            "Epoch [12/20], Loss: 2.2666\n",
            "Epoch [13/20], Loss: 2.2619\n",
            "Epoch [14/20], Loss: 2.2582\n",
            "Epoch [15/20], Loss: 2.2539\n",
            "Epoch [16/20], Loss: 2.2488\n",
            "Epoch [17/20], Loss: 2.2424\n",
            "Epoch [18/20], Loss: 2.2399\n",
            "Epoch [19/20], Loss: 2.2346\n",
            "Epoch [20/20], Loss: 2.2285\n",
            "\n",
            "Training finished!\n"
          ]
        }
      ],
      "source": [
        "print(\"\\nStarting training...\")\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    total_loss = 0\n",
        "    for i, (batch_inputs, batch_targets) in enumerate(data_loader):\n",
        "        # Move data to the primary device. DataParallel will scatter it.\n",
        "        batch_inputs = batch_inputs.to(device)\n",
        "        batch_targets = batch_targets.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        # DataParallel automatically splits the batch, sends it to the GPUs,\n",
        "        # executes the forward pass, and gathers the outputs on the primary device.\n",
        "        debug = epoch == 0 and i == 0\n",
        "        outputs = model(batch_inputs, debug=debug)\n",
        "        if debug:\n",
        "            print(\"Outside: input size\", batch_inputs.size(), \"output_size\", outputs.size())\n",
        "        loss = criterion(outputs, batch_targets)\n",
        "\n",
        "        # Backward and optimize\n",
        "        # The loss is computed on the primary GPU. The backward pass calculates\n",
        "        # gradients on each GPU, which are then summed on the primary GPU.\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    avg_loss = total_loss / len(data_loader)\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}')\n",
        "\n",
        "print(\"\\nTraining finished!\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "00949f04",
      "metadata": {
        "id": "00949f04",
        "lines_to_next_cell": 0
      },
      "source": [
        "## 5. Accessing the Original Model\n",
        "If you need to save the model's state dict or access the original model\n",
        "without the DataParallel wrapper, you need to use .module"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "5123ced2",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-06-18T14:16:32.162022Z",
          "iopub.status.busy": "2025-06-18T14:16:32.161780Z",
          "iopub.status.idle": "2025-06-18T14:16:32.169906Z",
          "shell.execute_reply": "2025-06-18T14:16:32.169166Z",
          "shell.execute_reply.started": "2025-06-18T14:16:32.162000Z"
        },
        "id": "5123ced2",
        "outputId": "1002f67a-79d6-4fe4-cfc6-35c777fd9538",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Model was not wrapped. Saving the model directly.\n"
          ]
        }
      ],
      "source": [
        "if isinstance(model, nn.DataParallel):\n",
        "    original_model = model.module\n",
        "    print(\"\\nModel was wrapped in DataParallel. Accessing the original model via .module\")\n",
        "    torch.save(original_model.state_dict(), 'model_state.pth')\n",
        "else:\n",
        "    original_model = model\n",
        "    print(\"\\nModel was not wrapped. Saving the model directly.\")\n",
        "    torch.save(original_model.state_dict(), 'model_state.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "44c03c01",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "e781bb85",
      "metadata": {},
      "source": [
        "#### Distributed Data Parallel (DDP) Tutorial Series"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "d0b86083",
      "metadata": {},
      "outputs": [],
      "source": [
        "import argparse                     # For parsing command-line arguments (though unused here)\n",
        "import os                           # For interacting with the operating system (e.g., env vars, paths)\n",
        "import sys                          # For system-specific parameters and functions (e.g., exit)\n",
        "import tempfile                     # For creating temporary files/directories (used on Windows)\n",
        "from urllib.parse import urlparse   # For parsing URL-style init_method strings\n",
        "\n",
        "import torch                        # Core PyTorch library\n",
        "import torch.distributed as dist    # PyTorch distributed communication package\n",
        "import torch.nn as nn               # Neural network modules\n",
        "import torch.optim as optim         # Optimization algorithms (e.g., SGD)\n",
        "\n",
        "from torch.nn.parallel import DistributedDataParallel as DDP  # Wrapper for model parallelism\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "184bf31e",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "False\n",
            "True\n"
          ]
        }
      ],
      "source": [
        "def verify_min_gpu_count(min_gpus: int = 2) -> bool:\n",
        "    \"\"\" verification that we have at least 2 gpus to run dist examples \"\"\"\n",
        "    has_gpu = torch.accelerator.is_available()              # Check if any accelerator (GPU) is available\n",
        "    gpu_count = torch.accelerator.device_count()            # Get number of available accelerators\n",
        "    return has_gpu and gpu_count >= min_gpus                # Return True if enough GPUs exist\n",
        "\n",
        "print(verify_min_gpu_count(2))\n",
        "print(verify_min_gpu_count(1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "12ff14c6",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "24388"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "os.getpid()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "3063e5ee",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "World Size (Number of GPUs): 1\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "world_size = torch.cuda.device_count()\n",
        "print(f\"World Size (Number of GPUs): {world_size}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ca81fb12",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "jupytext": {
      "cell_metadata_filter": "-all",
      "main_language": "python",
      "notebook_metadata_filter": "-all"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [],
      "dockerImageVersionId": 31041,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
