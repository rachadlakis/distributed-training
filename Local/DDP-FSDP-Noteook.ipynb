{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4af0b4be",
      "metadata": {},
      "outputs": [],
      "source": [
        "# uv init\n",
        "# uv venv torch_env\n",
        "# uv venv torch_gpu --python=3.13\n",
        "# uv venv torch_gpu --python \"C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python313\\python.exe\"\n",
        "\n",
        "# torch_env\\Scripts\\activate\n",
        "# uv add matplotlib pycocotools opencv-python Pillow torch ultralytics rfdetr pyyaml ipykernel\n",
        "\n",
        "## Had to use pip, uv still have some issues with GPU \n",
        "# uv pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
        "# uv pip install ultralytics\n",
        "# uv pip install rfdetr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "37c0c0a3",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "ed73a758",
      "metadata": {},
      "source": [
        "#### Single GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2853f11f",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "import os\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from datautils import MyTrainDataset\n",
        "from utils import print_nvidia_smi\n",
        "\n",
        "\n",
        "class Trainer:\n",
        "    def __init__(\n",
        "        self,\n",
        "        model: torch.nn.Module,\n",
        "        train_dataloader: DataLoader,\n",
        "        optimizer: torch.optim.Optimizer,\n",
        "        gpu_id: int,\n",
        "        save_every: int,\n",
        "    ) -> None:\n",
        "        self.model = model.to(gpu_id)\n",
        "        self.train_dataloader = train_dataloader\n",
        "        self.optimizer = optimizer        \n",
        "        self.gpu_id = gpu_id\n",
        "        self.save_every = save_every\n",
        "\n",
        "    def _run_batch(self, source, targets):\n",
        "        self.optimizer.zero_grad()\n",
        "        output = self.model(source)\n",
        "        loss = F.cross_entropy(output, targets)\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "    def _run_epoch(self, epoch):\n",
        "        # b_sz = len(next(iter(self.train_dataloader))[0]) \n",
        "        # print(f\"[GPU{self.gpu_id}] Epoch {epoch} | Batchsize: {b_sz} | Steps: {len(self.train_dataloader)}\")\n",
        "        print(f\"[GPU{self.gpu_id}] Epoch {epoch} | Steps: {len(self.train_dataloader)}\")\n",
        "        for source, targets in self.train_dataloader:\n",
        "            source = source.to(self.gpu_id)\n",
        "            targets = targets.to(self.gpu_id)\n",
        "            self._run_batch(source, targets)\n",
        "\n",
        "    def _save_checkpoint(self, epoch):\n",
        "        ckp = self.model.state_dict()\n",
        "        # PATH = \"final_checkpoint.pt\"\n",
        "        if not os.path.exists(\"trained_models\"):\n",
        "            os.makedirs(\"trained_models\")\n",
        "        PATH = f\"trained_models/checkpoint_epoch_{epoch}.pt\"\n",
        "        torch.save(ckp, PATH)\n",
        "        print(f\"Epoch {epoch} | Training checkpoint saved at {PATH}\")\n",
        "\n",
        "    def train(self, max_epochs: int):\n",
        "        for epoch in range(max_epochs):\n",
        "            self._run_epoch(epoch)\n",
        "            if epoch % self.save_every == 0:\n",
        "                self._save_checkpoint(epoch)\n",
        "\n",
        "\n",
        "def load_train_objs():\n",
        "    train_dataset = MyTrainDataset(2048)\n",
        "    model = torch.nn.Linear(20, 1)\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
        "    return train_dataset, model, optimizer\n",
        "\n",
        "\n",
        "def prepare_dataloader(dataset: Dataset, batch_size: int):\n",
        "    return DataLoader(\n",
        "        dataset,\n",
        "        batch_size=batch_size,\n",
        "        pin_memory=True,\n",
        "        shuffle=True\n",
        "    )\n",
        "\n",
        "\n",
        "def main(device, total_epochs, save_every, batch_size):\n",
        "    #check if GPU is available\n",
        "    if not torch.cuda.is_available():\n",
        "        raise RuntimeError(\"GPU is not available. This script requires a GPU to run.\")\n",
        "    else:\n",
        "        print(\"GPU is available. Proceeding with training on GPU.\")\n",
        "\n",
        "    dataset, model, optimizer = load_train_objs()\n",
        "    train_dataloader = prepare_dataloader(dataset, batch_size)\n",
        "    trainer = Trainer(model, train_dataloader, optimizer, device, save_every)\n",
        "    trainer.train(total_epochs)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\": \n",
        "    import argparse\n",
        "    parser = argparse.ArgumentParser(description='simple distributed training job')\n",
        "    parser.add_argument('--total_epochs', type=int, default=10, help='Total epochs to train the model')\n",
        "    parser.add_argument('--save_every', type=int, default=2, help='How often to save a snapshot')\n",
        "    parser.add_argument('--batch_size', type=int, default=32, help='Input batch size on each device (default: 32)')\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    print_nvidia_smi()\n",
        "    #     \n",
        "    device = 0  # shorthand for cuda:0\n",
        "    main(device, args.total_epochs, args.save_every, args.batch_size)\n",
        "\n",
        "## Usage example: \n",
        "# py 1-single_gpu.py --total_epochs 10 --save_every 2 --batch_size 64\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "040f56dc",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "97c2ce54",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "879dc33e",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Scheme: https\n",
            "Netloc: www.example.com:8080\n",
            "Hostname: www.example.com\n",
            "Port: 8080\n",
            "Path: /path/to/page\n",
            "Query: query=123&sort=asc\n",
            "Fragment: section2\n"
          ]
        }
      ],
      "source": [
        "from urllib.parse import urlparse\n",
        "\n",
        "\n",
        "url = \"https://www.example.com:8080/path/to/page?query=123&sort=asc#section2\"\n",
        "\n",
        "parsed = urlparse(url)\n",
        "\n",
        "print(\"Scheme:\", parsed.scheme)\n",
        "print(\"Netloc:\", parsed.netloc)\n",
        "print(\"Hostname:\", parsed.hostname)\n",
        "print(\"Port:\", parsed.port)\n",
        "print(\"Path:\", parsed.path)\n",
        "print(\"Query:\", parsed.query)\n",
        "print(\"Fragment:\", parsed.fragment)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "e5e77aa0",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "https\n"
          ]
        }
      ],
      "source": [
        "print(parsed.scheme)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fec22d55",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "905587c3",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "dd9b8ef0-5d69-44b7-8da9-10ec7306dd4d",
      "metadata": {
        "id": "dd9b8ef0-5d69-44b7-8da9-10ec7306dd4d"
      },
      "source": [
        "# DataParallel Notebook\n",
        "-------------------------\n",
        "Inspired by: [Pytorch Data Parallelism Tutorial](https://docs.pytorch.org/tutorials/beginner/blitz/data_parallel_tutorial.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "34c98ebd",
      "metadata": {
        "id": "34c98ebd",
        "lines_to_next_cell": 2
      },
      "source": [
        "# Imports and Initial Setup\n",
        "------------------------------------\n",
        "This cell imports the necessary PyTorch libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "6db1e4d5",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-06-18T14:30:01.357274Z",
          "iopub.status.busy": "2025-06-18T14:30:01.356521Z",
          "iopub.status.idle": "2025-06-18T14:30:01.360775Z",
          "shell.execute_reply": "2025-06-18T14:30:01.359986Z",
          "shell.execute_reply.started": "2025-06-18T14:30:01.357249Z"
        },
        "id": "6db1e4d5",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import TensorDataset, DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "d6af9aca",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-06-18T14:30:21.610011Z",
          "iopub.status.busy": "2025-06-18T14:30:21.609490Z",
          "iopub.status.idle": "2025-06-18T14:30:21.615075Z",
          "shell.execute_reply": "2025-06-18T14:30:21.614246Z",
          "shell.execute_reply.started": "2025-06-18T14:30:21.609987Z"
        },
        "id": "d6af9aca",
        "lines_to_next_cell": 1,
        "outputId": "e0ebb557-e0c2-461c-fc16-169c04cb816d",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PyTorch Version: 2.9.0+cu128\n",
            "------------------------------\n",
            "Found 1 GPUs.\n"
          ]
        }
      ],
      "source": [
        "print(f\"PyTorch Version: {torch.__version__}\")\n",
        "print(\"-\" * 30)\n",
        "\n",
        "# Check for available GPUs\n",
        "if torch.cuda.is_available():\n",
        "    num_gpus = torch.cuda.device_count()\n",
        "    print(f\"Found {num_gpus} GPUs.\")\n",
        "    # Set the primary device\n",
        "    device = torch.device(\"cuda:0\")\n",
        "else:\n",
        "    num_gpus = 0\n",
        "    print(\"No GPUs found. Running on CPU.\")\n",
        "    device = torch.device(\"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0c00bc52-470a-4526-826e-2216ff467778",
      "metadata": {
        "id": "0c00bc52-470a-4526-826e-2216ff467778"
      },
      "source": [
        "# Data Parallel\n",
        "-----------------------------\n",
        "Source: [DataParallel vs. DistributedDataParallel in PyTorch: Whatâ€™s the Difference?](https://medium.com/@mlshark/dataparallel-vs-distributeddataparallel-in-pytorch-whats-the-difference-0af10bb43bc7)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b7715eb1",
      "metadata": {
        "id": "b7715eb1"
      },
      "source": [
        "# Define a Simple Model\n",
        "-----------------------------\n",
        "We'll create a basic neural network for this demonstration.\n",
        "DataParallel will replicate this model on each available GPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "137aa2ce",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-06-18T14:33:51.487053Z",
          "iopub.status.busy": "2025-06-18T14:33:51.486118Z",
          "iopub.status.idle": "2025-06-18T14:33:51.491916Z",
          "shell.execute_reply": "2025-06-18T14:33:51.491377Z",
          "shell.execute_reply.started": "2025-06-18T14:33:51.487021Z"
        },
        "id": "137aa2ce",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "class SimpleModel(nn.Module):\n",
        "    def __init__(self, input_size, output_size):\n",
        "        super(SimpleModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, 128)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(128, output_size)\n",
        "\n",
        "    def forward(self, x, debug=False):\n",
        "        out = self.fc1(x)\n",
        "        out = self.relu(out)\n",
        "        out = self.fc2(out)\n",
        "        if debug:\n",
        "            print(\"\\tInside the Model: input size\", x.size(), \"output size\", out.size())\n",
        "        return out\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "66d4b247",
      "metadata": {
        "id": "66d4b247",
        "lines_to_next_cell": 2
      },
      "source": [
        "# Data Preparation and Training Loop\n",
        "------------------------------------------\n",
        "This is the main part where we wrap our model with DataParallel\n",
        "and run the training process."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "84a4dd7e",
      "metadata": {
        "id": "84a4dd7e",
        "lines_to_next_cell": 0
      },
      "source": [
        "## 1. Hyperparameters and Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "113e6748",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-06-18T14:35:14.851088Z",
          "iopub.status.busy": "2025-06-18T14:35:14.850315Z",
          "iopub.status.idle": "2025-06-18T14:35:14.924285Z",
          "shell.execute_reply": "2025-06-18T14:35:14.923505Z",
          "shell.execute_reply.started": "2025-06-18T14:35:14.851066Z"
        },
        "id": "113e6748",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "input_size = 784\n",
        "output_size = 10\n",
        "batch_size = 256  # A larger batch size helps utilize multiple GPUs\n",
        "learning_rate = 0.01\n",
        "num_epochs = 20\n",
        "\n",
        "# Create dummy data\n",
        "# We create a dataset of 10000 samples\n",
        "inputs = torch.randn(10000, input_size)\n",
        "targets = torch.randint(0, output_size, (10000,))\n",
        "\n",
        "# Use DataLoader for batching\n",
        "dataset = TensorDataset(inputs, targets)\n",
        "# The batch size will be split across GPUs. If you have 2 GPUs,\n",
        "# each will process batch_size / 2 samples.\n",
        "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "23f49f33",
      "metadata": {
        "id": "23f49f33",
        "lines_to_next_cell": 0
      },
      "source": [
        "## 2. Initialize and Wrap the Model\n",
        "Instantiate the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "86c5fc36",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-06-18T14:36:20.905451Z",
          "iopub.status.busy": "2025-06-18T14:36:20.904773Z",
          "iopub.status.idle": "2025-06-18T14:36:20.913077Z",
          "shell.execute_reply": "2025-06-18T14:36:20.912464Z",
          "shell.execute_reply.started": "2025-06-18T14:36:20.905423Z"
        },
        "id": "86c5fc36",
        "outputId": "4ac456d3-b301-40da-f473-246346a973f9",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training on a single device (CPU or 1 GPU).\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "SimpleModel(\n",
              "  (fc1): Linear(in_features=784, out_features=128, bias=True)\n",
              "  (relu): ReLU()\n",
              "  (fc2): Linear(in_features=128, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = SimpleModel(input_size, output_size)\n",
        "\n",
        "# IMPORTANT: Wrap the model with nn.DataParallel\n",
        "# This is the key step for data parallelism.\n",
        "# If multiple GPUs are available, this wrapper will handle the data distribution.\n",
        "if num_gpus > 1:\n",
        "    print(f\"Using {num_gpus} GPUs for training!\")\n",
        "    device_ids = list(range(num_gpus))  # Explicitly specify all available GPUs\n",
        "    model = nn.DataParallel(model, device_ids=device_ids)\n",
        "else:\n",
        "    print(\"Training on a single device (CPU or 1 GPU).\")\n",
        "\n",
        "# Move the model to the primary device. DataParallel will handle the rest.\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "577fc2b9",
      "metadata": {
        "id": "577fc2b9",
        "lines_to_next_cell": 0
      },
      "source": [
        "## 3. Loss and Optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "95b25c7b",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-06-18T14:36:30.917424Z",
          "iopub.status.busy": "2025-06-18T14:36:30.917136Z",
          "iopub.status.idle": "2025-06-18T14:36:30.921335Z",
          "shell.execute_reply": "2025-06-18T14:36:30.920770Z",
          "shell.execute_reply.started": "2025-06-18T14:36:30.917405Z"
        },
        "id": "95b25c7b",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5ceddd76",
      "metadata": {
        "id": "5ceddd76",
        "lines_to_next_cell": 0
      },
      "source": [
        "## 4. Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "96086fd4",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-06-18T14:37:56.528430Z",
          "iopub.status.busy": "2025-06-18T14:37:56.527863Z",
          "iopub.status.idle": "2025-06-18T14:38:00.899463Z",
          "shell.execute_reply": "2025-06-18T14:38:00.898449Z",
          "shell.execute_reply.started": "2025-06-18T14:37:56.528408Z"
        },
        "id": "96086fd4",
        "outputId": "ec2d2032-7f21-46ee-c7ab-c913831b14e3",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Starting training...\n",
            "\tInside the Model: input size torch.Size([256, 784]) output size torch.Size([256, 10])\n",
            "Outside: input size torch.Size([256, 784]) output_size torch.Size([256, 10])\n",
            "Epoch [1/20], Loss: 2.3264\n",
            "Epoch [2/20], Loss: 2.3170\n",
            "Epoch [3/20], Loss: 2.3090\n",
            "Epoch [4/20], Loss: 2.3047\n",
            "Epoch [5/20], Loss: 2.2997\n",
            "Epoch [6/20], Loss: 2.2965\n",
            "Epoch [7/20], Loss: 2.2870\n",
            "Epoch [8/20], Loss: 2.2838\n",
            "Epoch [9/20], Loss: 2.2795\n",
            "Epoch [10/20], Loss: 2.2735\n",
            "Epoch [11/20], Loss: 2.2701\n",
            "Epoch [12/20], Loss: 2.2666\n",
            "Epoch [13/20], Loss: 2.2619\n",
            "Epoch [14/20], Loss: 2.2582\n",
            "Epoch [15/20], Loss: 2.2539\n",
            "Epoch [16/20], Loss: 2.2488\n",
            "Epoch [17/20], Loss: 2.2424\n",
            "Epoch [18/20], Loss: 2.2399\n",
            "Epoch [19/20], Loss: 2.2346\n",
            "Epoch [20/20], Loss: 2.2285\n",
            "\n",
            "Training finished!\n"
          ]
        }
      ],
      "source": [
        "print(\"\\nStarting training...\")\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    total_loss = 0\n",
        "    for i, (batch_inputs, batch_targets) in enumerate(data_loader):\n",
        "        # Move data to the primary device. DataParallel will scatter it.\n",
        "        batch_inputs = batch_inputs.to(device)\n",
        "        batch_targets = batch_targets.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        # DataParallel automatically splits the batch, sends it to the GPUs,\n",
        "        # executes the forward pass, and gathers the outputs on the primary device.\n",
        "        debug = epoch == 0 and i == 0\n",
        "        outputs = model(batch_inputs, debug=debug)\n",
        "        if debug:\n",
        "            print(\"Outside: input size\", batch_inputs.size(), \"output_size\", outputs.size())\n",
        "        loss = criterion(outputs, batch_targets)\n",
        "\n",
        "        # Backward and optimize\n",
        "        # The loss is computed on the primary GPU. The backward pass calculates\n",
        "        # gradients on each GPU, which are then summed on the primary GPU.\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    avg_loss = total_loss / len(data_loader)\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}')\n",
        "\n",
        "print(\"\\nTraining finished!\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "00949f04",
      "metadata": {
        "id": "00949f04",
        "lines_to_next_cell": 0
      },
      "source": [
        "## 5. Accessing the Original Model\n",
        "If you need to save the model's state dict or access the original model\n",
        "without the DataParallel wrapper, you need to use .module"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "5123ced2",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-06-18T14:16:32.162022Z",
          "iopub.status.busy": "2025-06-18T14:16:32.161780Z",
          "iopub.status.idle": "2025-06-18T14:16:32.169906Z",
          "shell.execute_reply": "2025-06-18T14:16:32.169166Z",
          "shell.execute_reply.started": "2025-06-18T14:16:32.162000Z"
        },
        "id": "5123ced2",
        "outputId": "1002f67a-79d6-4fe4-cfc6-35c777fd9538",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Model was not wrapped. Saving the model directly.\n"
          ]
        }
      ],
      "source": [
        "if isinstance(model, nn.DataParallel):\n",
        "    original_model = model.module\n",
        "    print(\"\\nModel was wrapped in DataParallel. Accessing the original model via .module\")\n",
        "    torch.save(original_model.state_dict(), 'model_state.pth')\n",
        "else:\n",
        "    original_model = model\n",
        "    print(\"\\nModel was not wrapped. Saving the model directly.\")\n",
        "    torch.save(original_model.state_dict(), 'model_state.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "44c03c01",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "e781bb85",
      "metadata": {},
      "source": [
        "#### Distributed Data Parallel (DDP) Tutorial Series"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "d0b86083",
      "metadata": {},
      "outputs": [],
      "source": [
        "import argparse                     # For parsing command-line arguments (though unused here)\n",
        "import os                           # For interacting with the operating system (e.g., env vars, paths)\n",
        "import sys                          # For system-specific parameters and functions (e.g., exit)\n",
        "import tempfile                     # For creating temporary files/directories (used on Windows)\n",
        "from urllib.parse import urlparse   # For parsing URL-style init_method strings\n",
        "\n",
        "import torch                        # Core PyTorch library\n",
        "import torch.distributed as dist    # PyTorch distributed communication package\n",
        "import torch.nn as nn               # Neural network modules\n",
        "import torch.optim as optim         # Optimization algorithms (e.g., SGD)\n",
        "\n",
        "from torch.nn.parallel import DistributedDataParallel as DDP  # Wrapper for model parallelism\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "184bf31e",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "False\n",
            "True\n"
          ]
        }
      ],
      "source": [
        "def verify_min_gpu_count(min_gpus: int = 2) -> bool:\n",
        "    \"\"\" verification that we have at least 2 gpus to run dist examples \"\"\"\n",
        "    has_gpu = torch.accelerator.is_available()              # Check if any accelerator (GPU) is available\n",
        "    gpu_count = torch.accelerator.device_count()            # Get number of available accelerators\n",
        "    return has_gpu and gpu_count >= min_gpus                # Return True if enough GPUs exist\n",
        "\n",
        "print(verify_min_gpu_count(2))\n",
        "print(verify_min_gpu_count(1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "12ff14c6",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "24388"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "os.getpid()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "3063e5ee",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "World Size (Number of GPUs): 1\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "world_size = torch.cuda.device_count()\n",
        "print(f\"World Size (Number of GPUs): {world_size}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ca81fb12",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "jupytext": {
      "cell_metadata_filter": "-all",
      "main_language": "python",
      "notebook_metadata_filter": "-all"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [],
      "dockerImageVersionId": 31041,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "distributed-training (3.12.10)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
